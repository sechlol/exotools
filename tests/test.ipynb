{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T20:24:56.734009Z",
     "start_time": "2025-07-27T20:24:56.730257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.table import QTable\n",
    "from pyvo.io.vosi.vodataservice import VODataServiceTable\n",
    "from exotools.utils.qtable_utils import _get_qtable_paths, _read_qtable_header, QTableHeader, RootQTableHeader\n",
    "from exotools import CandidateExoplanetsDataset, TessDataset, LightcurveDataset, KnownExoplanetsDataset\n",
    "from exotools.downloaders.tap_service import ExoService\n",
    "from exotools.utils.qtable_utils import get_header_from_table\n",
    "from tests.utils import compare_qtables\n",
    "from astropy.time import Time\n",
    "from exotools.io.fs_storage import EcsvStorage\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "p = Path(\"/Users/christian/git/exotools/tests/tmp\")"
   ],
   "id": "1b08af5c308b83d0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T20:23:50.793258Z",
     "start_time": "2025-07-27T20:23:30.324723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_data():\n",
    "    known = KnownExoplanetsDataset(p).download_known_exoplanets(limit=15)\n",
    "    candidates = CandidateExoplanetsDataset(p).download_candidate_exoplanets(limit=15)\n",
    "    all_ids = np.concatenate([known.unique_ids, candidates.unique_ids])\n",
    "    tess_meta = TessDataset(p).download_observation_metadata(targets_tic_id=all_ids, store=True)\n",
    "    LightcurveDataset(p).download_lightcurves_from_tess_db(tess_meta)\n",
    "\n",
    "\n",
    "# download_data()\n",
    "exo_qtable = KnownExoplanetsDataset(p).download_known_exoplanets(limit=15, with_gaia_star_data=False).view"
   ],
   "id": "a9fbfc1be60424d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to download known exoplanets dataset...\n",
      "Querying https://exoplanetarchive.ipac.caltech.edu/TAP/ (synchronous)...\n",
      "DONE! Collected 15 unique planets, for a total of 15 records.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T20:25:47.123688Z",
     "start_time": "2025-07-27T20:25:46.572407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exo_header = _read_qtable_header(p/\"known_exoplanets_header.json\")\n",
    "ecsv = EcsvStorage(root_path=p/\"ecsv\")\n",
    "ecsv.write_qtable(exo_qtable, exo_header, \"known\", True)\n",
    "ecsv_read = ecsv.read_qtable(\"known\")\n",
    "compare_qtables(exo_qtable, ecsv_read)"
   ],
   "id": "2db905b49c514403",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from astropy.table import MaskedColumn\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "\n",
    "def save_qtable(\n",
    "    table: QTable,\n",
    "    header: Optional[QTableHeader],\n",
    "    file_path: Path,\n",
    "    file_name: Optional[str] = None,\n",
    ") -> Path:\n",
    "    data_path, header_path = _get_qtable_paths(file_path, file_name)\n",
    "    data_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if header is not None:\n",
    "        root_model = RootQTableHeader(root=header)\n",
    "\n",
    "        # Store table unit info in json format\n",
    "        with open(header_path, \"w\") as f:\n",
    "            f.write(root_model.model_dump_json(indent=4))\n",
    "\n",
    "    # Store table data in feather format\n",
    "    df = table.to_pandas().reset_index()\n",
    "    if \"index\" in df:\n",
    "        df = df.drop(columns=\"index\")\n",
    "    df.to_feather(data_path)\n",
    "\n",
    "    return data_path\n",
    "\n",
    "def read_qtable(file_path: Path, file_name: Optional[str] = None) -> QTable:\n",
    "    data_path, header_path = _get_qtable_paths(file_path=file_path, file_name=file_name)\n",
    "\n",
    "    # Read header information with column units\n",
    "    header = _read_qtable_header(header_path)\n",
    "    units = {key: info.unit for key, info in header.items()} if header else None\n",
    "\n",
    "    # Read data and assign units\n",
    "    if not data_path.exists():\n",
    "        raise ValueError(f\"read_qtable(): given path does not exist: {data_path}\")\n",
    "\n",
    "    df = pd.read_feather(data_path)\n",
    "    qtable = QTable.from_pandas(df, units=units)\n",
    "    for c in qtable.columns:\n",
    "        qtable[c].description = header[c].description if c in header else None\n",
    "\n",
    "    return qtable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p_coniglio = p / \"coniglio\"\n",
    "\n",
    "exo_qtable.write(p_coniglio/\"coniglio.ecsv\", overwrite=True, serialize_method={MaskedColumn: 'data_mask'})\n",
    "ecsv_read = QTable.read(p_coniglio/\"coniglio.ecsv\")\n",
    "compare_qtables(exo_qtable, ecsv_read)\n",
    "\n",
    "save_qtable(exo_qtable, h, p_coniglio, \"coniglio\")\n",
    "coniglio_table = read_qtable(file_path=p_coniglio, file_name=\"coniglio\")\n",
    "# compare_qtables(exo_qtable, coniglio_table)\n",
    "# ExoDB.preprocess_dataset(a)\n",
    "# ExoDB.compute_bounds(a)\n",
    "\n",
    "# It's useful to disable parsing Time columns if we need them as Quantities,\n",
    "# for example to copy units to another qtable.\n",
    "# if convert_time_columns:\n",
    "#     ExoDB.convert_time_columns(a)\n"
   ],
   "id": "2aaf5826e5f668a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hdf5_wrapper = Hdf5Wrapper(Path(\"/Users/christian/git/exotools/tests/tmp/dataset.hdf5\"))\n",
    "\n",
    "hdf5_wrapper.write_qtable(tess_meta.view, get_header_from_table(tess_meta.view), \"tess_meta\")\n",
    "hdf5_wrapper.write_qtable(candidate_db.view, get_header_from_table(candidate_db.view), \"candidate_db\")\n",
    "hdf5_wrapper.write_qtable(exo_db.view, get_header_from_table(exo_db.view), \"exo_db\")\n",
    "hdf5_wrapper.write_qtable(lc_db.view, get_header_from_table(lc_db.view), \"lc_db\")\n",
    "hdf5_wrapper.write_json(data={\"dio\": \"stra_porco\"}, name=\"porco\")\n",
    "hdf5_wrapper.write_json(data={\"dio\": \"megamerda\"}, name=\"merda\")\n"
   ],
   "id": "70224ca31ba5f713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "exo_qtable.write(\"coniglio.hdf5\", path=\"coniglio\", overwrite=True, append=True, compression=True, serialize_meta=True)\n",
    "coniglio = QTable.read(\"coniglio.hdf5\", path=\"coniglio\")\n",
    "compare_qtables(exo_qtable, coniglio)"
   ],
   "id": "6bb221cc8acfa3cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "for c in exo_qtable.itercols():\n",
    "    if isinstance(c, Time):\n",
    "        continue\n",
    "    if c.dtype == \"O\":\n",
    "        print(c.name)"
   ],
   "id": "d3992fd6ec622fa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "exoservice = ExoService()\n",
    "ps_table : VODataServiceTable = exoservice._service.tables[\"ps\"]\n",
    "ps_table"
   ],
   "id": "9fec93490dec4b94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def _get_fields_info(table) -> QTableHeader:\n",
    "#     return {\n",
    "#         # NOTE: column.name was previously column.feature_name, after this method started to fail for GAIA data downloader\n",
    "#         column.name: TableColumnInfo(unit=column.unit, description=column.description)\n",
    "#         for column in table.columns\n",
    "#     }\n",
    "aaa = []\n",
    "for a in ps_table.columns:\n",
    "    aaa.append([a.name, a.unit, a.datatype.content, a.description])\n",
    "gesu = pd.DataFrame(aaa, columns=[\"name\", \"unit\", \"datatype\", \"description\"])\n",
    "\n",
    "print(gesu.unit.unique())\n",
    "print(gesu.datatype.unique())"
   ],
   "id": "fcf3badd6eb58fcc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
